{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8845d6fb-fa7a-4166-a2df-89c6af2a9b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import math\n",
    "import gc #force garbage collection if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18377050-04ae-4597-b716-e085d32edeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program using mps\n"
     ]
    }
   ],
   "source": [
    "#find GPU device\n",
    "if torch.cuda.is_available():\n",
    "    #if Nvidia GPU available, use it\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    #if apple silicon available\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    #else just go with CPU\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Program using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a02afc72-30e4-46f6-a44d-db547154ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gate:\n",
    "    \n",
    "    def __init__(self, device, custom=None):\n",
    "        self.device = device\n",
    "        self.custom = custom\n",
    "        self.H = torch.tensor([[1., 1.],\n",
    "                               [1., -1.]], device=self.device, dtype=torch.cfloat) / torch.sqrt(torch.tensor(2))\n",
    "    \n",
    "        self.I = torch.tensor([[1., 0.],\n",
    "                               [0., 1.]], device=self.device, dtype=torch.cfloat)\n",
    "\n",
    "        self.S = torch.tensor([[1., 0.],\n",
    "                               [0., 1.j]], device=self.device, dtype=torch.cfloat)\n",
    "\n",
    "        #square root of X gate\n",
    "        self.SX= torch.tensor([[1.+1.j, 1.-1.j],\n",
    "                               [1.-1.j, 1.+1.j]], device=self.device, dtype=torch.cfloat) / 2\n",
    "\n",
    "        #Phase gate T uses class phase shift method (fourth root of Z)\n",
    "        self.T = self.P(torch.pi/4)\n",
    "        \n",
    "        self.X = torch.tensor([[0., 1.],\n",
    "                               [1., 0.]], device=self.device, dtype=torch.cfloat)\n",
    "\n",
    "        self.Y = torch.tensor([[0., -1.j],\n",
    "                               [1.j, 0.]], device=self.device, dtype=torch.cfloat)\n",
    "\n",
    "        self.Z = torch.tensor([[1., 0.],\n",
    "                               [0., -1.]], device=self.device, dtype=torch.cfloat)\n",
    "\n",
    "    #---Single qubit matrix operator---#\n",
    "    '''\n",
    "    Routine to apply given single-qubit 2x2 unitary matrix operator on a target \n",
    "    qubit index in some given states vector (representing the wavefunction) if the \n",
    "    number of qubits is greater than 16\n",
    "    As MPS cannot currently support tensors of rank >16, this is required to provide\n",
    "    a small speedup using MPS over CPU\n",
    "    '''\n",
    "\n",
    "    def apply_mps(self, gate, states, target):\n",
    "        N = states.numel()\n",
    "        if target < 0:\n",
    "            raise ValueError(\"Target cannot have negative index!\")\n",
    "        if N == 0:\n",
    "            raise ValueError(\"State vector cannot be empty!\")\n",
    "        if (N & (N - 1)) != 0:\n",
    "            raise ValueError(\"State vector length must be a power of two\")\n",
    "        n = int(math.log2(N))\n",
    "        if 2**(target + 1) > N:\n",
    "            raise ValueError(\"Target index cannot exceed circuit!\")\n",
    "        if gate.shape != torch.Size([2, 2]):\n",
    "            raise ValueError(\"Provided gate is not 2x2 unitary!\")\n",
    "            \n",
    "        #create mask to identify target qubit\n",
    "        mask = 1 << target\n",
    "        #range indices of wavefunction amplitudes\n",
    "        indices = torch.arange(states.numel())\n",
    "        #identify indices adjacent to target qubit index\n",
    "        i = indices[(indices & mask)==0]\n",
    "        j = i ^ mask\n",
    "        #apply operator\n",
    "        a0, a1 = states[i], states[j]\n",
    "        states[i] = gate[0,0] * a0 + gate[0,1] * a1\n",
    "        states[j] = gate[1,0] * a0 + gate[1,1] * a1\n",
    "        return states\n",
    "\n",
    "    '''\n",
    "    General apply routine for 2x2 unitary matrix operator on target qubit\n",
    "        '''\n",
    "    def apply(self, gate, state, target):\n",
    "        \"\"\"\n",
    "        High-performance strided single-qubit gate application.\n",
    "        Works on CPU, CUDA, and MPS.\n",
    "        Avoids high-rank tensors (MPS limit), avoids masked indexing (CUDA poison).\n",
    "        \"\"\"\n",
    "    \n",
    "        N = state.numel()\n",
    "        if target < 0:\n",
    "            raise ValueError(\"Target cannot have negative index\")\n",
    "        if N & (N - 1):\n",
    "            raise ValueError(\"State vector length must be power of two\")\n",
    "    \n",
    "        # stride between alternating |0> and |1> amplitudes of this target qubit\n",
    "        stride = 1 << target\n",
    "        block = stride << 1  # = 2 * stride\n",
    "    \n",
    "        # number of blocks\n",
    "        num_blocks = N // block\n",
    "    \n",
    "        # reshape into (num_blocks, stride) but flattened because we want 1D layout\n",
    "        # we will manually create views using strides\n",
    "        # indexes:\n",
    "        #   base + offset gives |0>\n",
    "        #   base + offset + stride gives |1>\n",
    "    \n",
    "        # we create views with appropriate strides â€” no memory copies\n",
    "        s = state  # alias\n",
    "    \n",
    "        # a0: all |0> amplitudes for this target qubit\n",
    "        a0 = s.as_strided(\n",
    "            size=(num_blocks, stride),\n",
    "            stride=(block, 1)\n",
    "        )\n",
    "    \n",
    "        # a1: all |1> amplitudes for this target qubit\n",
    "        a1 = s.as_strided(\n",
    "            size=(num_blocks, stride),\n",
    "            stride=(block, 1),\n",
    "            storage_offset=stride\n",
    "        )\n",
    "    \n",
    "        # Stack into (num_blocks * stride, 2) column vectors:\n",
    "        # But we want shape (2, M) to matmul with (2,2)\n",
    "        # So we do the transpose\n",
    "        M = num_blocks * stride\n",
    "    \n",
    "        v0 = a0.reshape(M)\n",
    "        v1 = a1.reshape(M)\n",
    "    \n",
    "        # Build a stacked (2, M) matrix WITHOUT allocating huge tensors\n",
    "        V = torch.stack((v0, v1), dim=0)  # shape (2, M)\n",
    "    \n",
    "        # Apply the 2x2 gate: (2,2) @ (2,M) -> (2,M)\n",
    "        out = gate @ V\n",
    "    \n",
    "        # Write results back in-place\n",
    "        v0.copy_(out[0])\n",
    "        v1.copy_(out[1])\n",
    "    \n",
    "        return state\n",
    "    #---Phase transforms---#\n",
    "    \n",
    "    '''\n",
    "    Ph (Global phase transform) takes a real-valued angle argument\n",
    "    representing a complex phase rotation in the Block sphere\n",
    "    The phase transform rotates the qubit state without changing its\n",
    "    probability of collapse to |0) or |1)\n",
    "    '''\n",
    "    def Ph(self, phase):\n",
    "        phase = phase % (2*torch.pi) #modulus 2*pi of input\n",
    "        Ph_angle = torch.tensor([0. + 1j*(phase)], device=self.device, dtype=torch.cfloat)\n",
    "        Ph_angle = torch.exp(Ph_angle)\n",
    "        return Ph_angle * self.I\n",
    "    '''\n",
    "    Bloch sphere rotation around the x axis\n",
    "    '''\n",
    "    def Rx(self, angle):\n",
    "        angle = torch.tensor(angle, device=self.device)\n",
    "        #form sin and cos components\n",
    "        cos_comp = torch.cos((angle/2))*self.I\n",
    "        sin_comp = 1j*torch.sin((angle/2))*self.X\n",
    "        #form complete matrix\n",
    "        Rx_rot = cos_comp - sin_comp\n",
    "        return Rx_rot\n",
    "    '''\n",
    "    Bloch sphere rotation around the y axis\n",
    "    '''\n",
    "    def Ry(self, angle):\n",
    "        angle = torch.tensor(angle, device=self.device)\n",
    "        #form sin and cos components\n",
    "        cos_comp = torch.cos((angle/2))*self.I\n",
    "        sin_comp = 1j*torch.sin((angle/2))*self.Y\n",
    "        #form complete matrix\n",
    "        Ry_rot = cos_comp - sin_comp\n",
    "        return Ry_rot\n",
    "    '''\n",
    "    Bloch sphere rotation around the z axis\n",
    "    '''\n",
    "    def Rz(self, angle):\n",
    "        angle = torch.tensor(angle, device=self.device)\n",
    "        #form sin and cos components\n",
    "        cos_comp = torch.cos(angle/2)*self.I\n",
    "        sin_comp = 1j*torch.sin(angle/2)*self.Z\n",
    "        #form complete matrix\n",
    "        Rz_rot = cos_comp - sin_comp\n",
    "        return Rz_rot\n",
    "    '''\n",
    "    Universal Bloch sphere rotation\n",
    "    U(theta, phi, lambda)\n",
    "    '''\n",
    "    def U(self, theta, phi, lam):\n",
    "        #convert angles to tensors\n",
    "        theta_t = torch.tensor(theta/2, device=self.device)\n",
    "        phi_t = torch.tensor(phi, device=self.device)\n",
    "        lam_t = torch.tensor(lam, device=self.device)\n",
    "        #form tensor using closed-form definition\n",
    "        U_rot = torch.tensor(\n",
    "            [[  torch.cos(theta_t),\n",
    "                -1*torch.exp(1.j*lam_t)*torch.sin(theta_t)\n",
    "             ],\n",
    "             [  torch.exp(1.j*phi_t)*torch.sin(theta_t),\n",
    "                torch.exp(1.j*(lam_t+phi_t))*torch.cos(theta_t)\n",
    "             ]], device=self.device)\n",
    "        return U_rot\n",
    "\n",
    "    #---Controlled gates---#\n",
    "    '''\n",
    "    These sets of operators are syntactically different than\n",
    "    2x2 and rotation operators as they must extend between\n",
    "    a control qubit and a target qubit\n",
    "    For a n-qubit system with target 4 and control 0, the\n",
    "    operator must extend across 5 qubits! Furthermore, the\n",
    "    matrix created for an n-qubit system holds 2^n x 2^n values\n",
    "    Since for all practical purposes this is vastly inefficient,\n",
    "    these routines use classical bitwise analogs instead of matrices\n",
    "    '''\n",
    "    def CNOT(self, states, control, target):\n",
    "        C = 1 << control\n",
    "        T = 1 << target\n",
    "        N = len(states)\n",
    "        if (control < 0) or (target < 0):\n",
    "            raise ValueError(\"Indices cannot be negative\")\n",
    "        elif (2**(control+1) > N) or (2**(target+1) > N):\n",
    "            raise ValueError(\"Control and target qubits must be in range\")\n",
    "        \n",
    "        #create indices tensor\n",
    "        indices = torch.arange(N)\n",
    "        #perform XOR on target and state indices\n",
    "        i = indices[(indices & C)!=0]\n",
    "        j = i^T\n",
    "        #swap states\n",
    "        tmp = states[i].clone()\n",
    "        states[i] = states[j]\n",
    "        states[j] = tmp\n",
    "        return states\n",
    "\n",
    "    #---Non-clifford gates---#\n",
    "    '''\n",
    "    These are the set of non-universal quantum gates\n",
    "    Since these are 2x2, they can still be applied using the apply() function\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    This a single amplitude phase shift routine\n",
    "    '''\n",
    "    def P(self, phi):\n",
    "        #convert angle to torch tensor and exponentiate\n",
    "        phi_t = torch.tensor(phi, device=self.device)\n",
    "        phi_t = torch.exp(1j*phi_t)\n",
    "        #form matrix as tensor\n",
    "        P_rot = torch.tensor([\n",
    "            [1, 0],\n",
    "            [0, phi_t]\n",
    "        ], dtype=torch.cfloat, device=self.device)\n",
    "        return P_rot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a2a8b84-cba9-4f23-a675-9145bbb9dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class circuit:\n",
    "    def __init__(self, size):\n",
    "        \n",
    "        #constructor takes size parameter describing number of qubits in circuit\n",
    "        self.size = size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "516b9289-b403-4915-895d-56491cc69e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS accelerated took 12.089386940002441 s\n",
      "CPU took 40.67214608192444 s\n",
      "Saved wavefunction states to saved_states/states.bin\n",
      "tensor([0.-0.5000j, 0.+0.5000j, 0.+0.0000j, 0.+0.0000j, 0.+0.0000j, 0.+0.0000j,\n",
      "        0.+0.0000j, 0.+0.0000j, 0.+0.0000j, 0.+0.0000j], device='mps:0')\n",
      "512.001953125\n"
     ]
    }
   ],
   "source": [
    "#test cases\n",
    "gate = Gate(device=device) #create gate object on mps\n",
    "gate_cpu = Gate(device=torch.device('cpu'))\n",
    "n = 26 #test with n qubits\n",
    "\n",
    "t0_gpu = time.time()\n",
    "x = torch.zeros(2**n, dtype=torch.cfloat, device=device)\n",
    "x[0] = 1\n",
    "for i in range(n):\n",
    "    x = gate.apply(gate.H, x, i)\n",
    "    x = gate.apply(gate.Y, x, i)\n",
    "    x = gate.apply(gate.Rx(torch.pi/2), x, i)\n",
    "t1_gpu = time.time()\n",
    "print(f\"MPS accelerated took {t1_gpu - t0_gpu} s\")\n",
    "\n",
    "t0_cpu = time.time()\n",
    "y = torch.zeros(2**n, dtype=torch.cfloat, device=torch.device('cpu'))\n",
    "y[0] = 1\n",
    "for i in range(n):\n",
    "    y = gate_cpu.apply(gate_cpu.H, y, i)\n",
    "    y = gate_cpu.apply(gate_cpu.Y, y, i)\n",
    "    y = gate_cpu.apply(gate_cpu.Rx(torch.pi/2), y, i)\n",
    "t1_cpu = time.time()\n",
    "print(f\"CPU took {t1_cpu - t0_cpu} s\")\n",
    "#print(x)\n",
    "save_states(x, \"saved_states/states.bin\")\n",
    "print(import_states(\"saved_states/states.bin\", device=device)[0:10])\n",
    "print(gpu_mem_usage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7526141d-2480-4c69-aa11-c1c455203078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_mem_usage():\n",
    "    if torch.cuda.is_available():\n",
    "        mem_used = torch.cuda.memory_allocated()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        mem_used = torch.mps.current_allocated_memory()\n",
    "    else:\n",
    "        print(\"Cannot show memory usage for CPU\")\n",
    "        return\n",
    "    mem_used /= 1024**2\n",
    "    return mem_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c63e4211-5f5b-4cfa-82f3-9136691228c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Save torch tensor directly to binary file\n",
    "'''\n",
    "def save_states(states, filename):\n",
    "    states.cpu().numpy().tofile(filename)\n",
    "    print(f\"Saved wavefunction states to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbc297d5-f63c-44ec-9132-c448a5adc761",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Read raw binary directly into torch tensor on device\n",
    "'''\n",
    "def import_states(filename, dtype=torch.cfloat, device='cpu'):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        buf = f.read()\n",
    "\n",
    "    tensor = torch.frombuffer(buf, dtype=dtype)\n",
    "    return tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036737c5-9cf4-4688-9b09-fd96520ed0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shenglong-qc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
